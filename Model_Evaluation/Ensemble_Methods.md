### ğŸ§  Ensemble Methods


**Ensemble methods** combine the predictions of **multiple models** to create a **stronger, more accurate model** than any single one alone.
Idea â†’ â€œMany weak learners together can make one strong learner.â€

    > Ensemble learning improves model performance by combining multiple models to reduce bias, variance, or both. Bagging decreases variance, boosting reduces bias, and stacking learns optimal model combinations.

---

### ğŸ¯ Why we use it:

* To **reduce overfitting** (variance).
* To **improve accuracy** and **stability**.
* To **handle complex patterns** that single models may miss.

---

### âš™ï¸ Main Types of Ensemble Methods:

| **Method**                          | **Concept**                                                                                                                                            | **Examples / Key Points**                                                                                             |
| ----------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------- |
| **Bagging (Bootstrap Aggregating)** | Trains multiple models (usually decision trees) on *different random subsets* of the data (using bootstrapping). Then averages or votes their results. | Example: **Random Forest** ğŸŒ³<br>âœ”ï¸ Reduces variance<br>âœ”ï¸ Good for unstable models like trees                        |
| **Boosting**                        | Trains models *sequentially*. Each new model focuses more on the errors of the previous one.                                                           | Examples: **AdaBoost**, **Gradient Boosting**, **XGBoost** ğŸš€<br>âœ”ï¸ Reduces bias<br>âœ”ï¸ Very powerful for tabular data |
| **Stacking**                        | Combines predictions of multiple different models (like SVM, Tree, Logistic Regression) using another â€œmeta-modelâ€ to learn how to best mix them.      | âœ”ï¸ Flexible<br>âœ”ï¸ Often gives top performance in competitions                                                         |

---

![advanced_ensemble_techniques](https://spotintelligence.com/wp-content/uploads/2024/03/bagging-boosting-stacking.jpg)

### ğŸ§© Example:

In Bagging â†’ you train 10 decision trees on different bootstrap samples.
Each tree gives a prediction, and the final output is decided by **majority voting** (for classification) or **average** (for regression).


---