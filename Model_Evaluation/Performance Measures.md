## üåü Classification Performance Measures

### üí≠ What is Classification?

Classification is an ML task where the model **predicts a category or class label** (e.g., spam vs not spam, disease vs healthy).

* You train it using labeled data (supervised learning).
* After training, you must **evaluate how well** your model performs ‚Äî that‚Äôs where *performance measures* come in.

---

## üß© 1. Confusion Matrix ‚Äî The Core Concept

Everything in classification metrics starts here üëá

| **Actual / Predicted** | **Positive**        | **Negative**        |
| ---------------------- | ------------------- | ------------------- |
| **Positive**           | True Positive (TP)  | False Negative (FN) |
| **Negative**           | False Positive (FP) | True Negative (TN)  |

üìò **Meaning:**

* **TP:** Model correctly predicts positive class.
* **TN:** Model correctly predicts negative class.
* **FP:** Model wrongly predicts positive (false alarm).
* **FN:** Model misses a positive (missed detection).

‚úÖ Example:

* Predicting ‚Äúdisease‚Äù

  * **TP:** Sick person correctly identified
  * **FP:** Healthy person marked as sick
  * **FN:** Sick person missed
  * **TN:** Healthy person correctly ignored

---

## ‚öñÔ∏è 2. Accuracy

**Formula:**
$$
Accuracy = \frac{TP + TN}{TP + TN + FP + FN}
$$

üß† **Meaning:** How often the classifier is correct overall.
üìç **Use:** Best for **balanced datasets** (equal positives & negatives).
üö´ **Problem:** Misleading when data is **imbalanced** (e.g., 99 healthy, 1 sick ‚Üí always predicting ‚Äúhealthy‚Äù gives 99% accuracy but is useless).

> ‚ÄúAccuracy is the ratio of correctly predicted observations to total observations. It works well for balanced datasets but fails in case of class imbalance.‚Äù

---

## üéØ 3. Precision

**Formula:**
$$
Precision = \frac{TP}{TP + FP}
$$

üß† **Meaning:** Of all items predicted positive, how many are actually positive.
üìç **Use:** Important when **false positives are costly** (e.g., spam detection ‚Äî we don‚Äôt want normal emails marked as spam).

> ‚ÄúPrecision measures the correctness of positive predictions and is useful when minimizing false positives is important.‚Äù

---

## üîç 4. Recall (Sensitivity or True Positive Rate)

**Formula:**
$$
Recall = \frac{TP}{TP + FN}
$$


üß† **Meaning:** Of all actual positives, how many did we correctly identify?
üìç **Use:** Important when **missing a positive is dangerous** (e.g., disease detection ‚Äî better to flag more people than miss a sick one).

> ‚ÄúRecall measures the model‚Äôs ability to detect positive instances. High recall means fewer false negatives.‚Äù

---

## ‚öñÔ∏è 5. F1-Score

**Formula:**
$$
F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}
$$

üß† **Meaning:** A balance between precision and recall.
üìç **Use:** When dataset is **imbalanced** and you want a single score summarizing both.

> ‚ÄúThe F1-Score is the harmonic mean of precision and recall. It is a more balanced measure than accuracy for imbalanced datasets.‚Äù

---

## üìà 6. ROC Curve (Receiver Operating Characteristic)

üß† **Concept:**

* A **graphical tool** to visualize classifier performance at all thresholds.
* X-axis ‚Üí False Positive Rate (FPR = FP / (FP + TN))
* Y-axis ‚Üí True Positive Rate (TPR = Recall = TP / (TP + FN))

üöÄ **Use:**

* Helps choose the best threshold for classification (trade-off between sensitivity & specificity).
* Useful when comparing multiple classifiers.

> ‚ÄúROC curve plots TPR vs FPR for different threshold values. The closer the curve is to the top-left corner, the better the classifier.‚Äù

---

## üßÆ 7. AUC (Area Under the Curve)

üß† **Meaning:**

* A single number summarizing ROC performance.
* **Range:** 0 to 1

  * 1 = Perfect model
  * 0.5 = Random guessing

üìç **Use:**
Compare models ‚Äî the one with higher AUC performs better.

> ‚ÄúAUC represents the probability that the classifier ranks a random positive instance higher than a random negative one.‚Äù

---

## üß† Summary Table

| Metric        | Formula                       | Use Case                   | Range |
| ------------- | ----------------------------- | -------------------------- | ----- |
| **Accuracy**  | (TP + TN)/(TP + TN + FP + FN) | Balanced data              | 0‚Äì1   |
| **Precision** | TP/(TP + FP)                  | Avoid false positives      | 0‚Äì1   |
| **Recall**    | TP/(TP + FN)                  | Avoid false negatives      | 0‚Äì1   |
| **F1-Score**  | 2*(P*R)/(P+R)                 | Balance P & R              | 0‚Äì1   |
| **AUC**       | Area under ROC                | Overall classifier ability | 0‚Äì1   |

---

## üß© Real-World Examples

| Scenario          | Best Metric                           |
| ----------------- | ------------------------------------- |
| Spam Detection    | **Precision** (avoid false positives) |
| Disease Detection | **Recall** (don‚Äôt miss positives)     |
| Credit Fraud      | **F1-Score** (balance both)           |

---
