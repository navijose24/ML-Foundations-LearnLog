# ‚≠ê **ACTIVATION FUNCTIONS**


# üí° **What is an Activation Function?**

An activation function decides whether a neuron should ‚Äúfire‚Äù or not.

It introduces **non-linearity** ‚Üí this allows neural networks to learn **complex patterns**.

Without activation functions ‚Üí your network becomes a simple linear model ‚Üí totally useless for real-world problems.

---

# üß† **Why We Need Activation Functions?**

Because real-world data is **non-linear**:

* Images
* Speech
* Emotions
* Natural language
* Business patterns

Activation functions create **curves and bends** so the network can learn the real patterns.

---

# ‚≠ê TYPES OF ACTIVATION FUNCTIONS


# 1Ô∏è‚É£ **Sigmoid Function (Logistic)**

**Formula:**

$f(x) = \frac{1}{1 + e^{-x}}$

**Range:** 0 to 1
**Shape:** S-shaped curve

![Sigmoid Function](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQovfM4UAyi_a07pWKNPQpX8_JFU5851Gk_VO8bMpWptb6GIh9YMlMhecNIFUNf3TbAVezNUuF_)

### **Where to use:**

‚úî Binary classification output layer

‚úî Probabilities (0‚Äì1)

### **Advantages:**

* Smooth curve
* Output is probability-like

### **Disadvantages:**

* Vanishing gradient problem
* Saturates at extreme ends
* Slow learning

---

# 2Ô∏è‚É£ **Tanh (Hyperbolic Tangent)**

**Formula:**

$f(x) = \tanh(x)$

**Range:** ‚Äì1 to +1


![Tanh (Hyperbolic Tangent)](https://miro.medium.com/v2/resize:fit:756/1*tOc--h-QU9_bHqWLPY9YLA.png)


### **Where to use:**

‚úî Hidden layers (better than sigmoid)

### **Advantages:**

* Zero-centered output
* Stronger gradient than sigmoid

### **Disadvantages:**

* Still has vanishing gradient

---

# 3Ô∏è‚É£ **ReLU (Rectified Linear Unit)**

**Formula:**

$f(x) = \max(0, x)$

![ReLu](https://cdn.prod.website-files.com/614c82ed388d53640613982e/64a6c16cff99b6337d9dacf7_parametric%20relu%20and%20first%20derivative.webp)

### **Where to use:**

‚úî All modern deep neural networks

‚úî CNNs, RNNs, Transformers

### **Advantages:**

* No vanishing gradient for x > 0
* Simple and fast
* Best for hidden layers

### **Disadvantages:**

* Dead ReLU problem (neurons stop updating if x < 0 always)

---

# 4Ô∏è‚É£ **Leaky ReLU**

Fixes ReLU‚Äôs ‚Äúdead neuron‚Äù problem.

**Formula:**
$$
f(x) =
\begin{cases}
x & x > 0 \
0.01x & x \le 0
\end{cases}
$$


![leaky ReLu](https://cdn.analyticsvidhya.com/wp-content/uploads/2017/10/prelu-300x262.png)

### **Where to use:**

‚úî When ReLU neurons die

‚úî Deep networks

---

# 5Ô∏è‚É£ **Softmax**

Used only in **multi-class classification (output layer)**.

**Formula:**

$\sigma(z_i)=\frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}$

![soft max](https://www.researchgate.net/publication/348703101/figure/fig5/AS:983057658040324@1611390618742/Graphic-representation-of-the-softmax-activation-function.ppm)

### **Where to use:**

‚úî Multi-class output (cat, dog, horse)

‚úî Probability distribution (sums to 1)

---

# üìå In short:

| Activation     | Range   | Use Case           | Pros                 | Cons                  |
| -------------- | ------- | ------------------ | -------------------- | --------------------- |
| **Sigmoid**    | 0 to 1  | Binary output      | Probability          | Vanishing gradient    |
| **Tanh**       | ‚Äì1 to 1 | Hidden layers      | Zero-centered        | Can still saturate    |
| **ReLU**       | 0 to ‚àû  | Hidden layers      | Fast, no VG          | Dead neurons          |
| **Leaky ReLU** | ‚Äì‚àû to ‚àû | Deep hidden layers | Fixes dead ReLU      | Slight noise          |
| **Softmax**    | 0‚Äì1     | Multi-class output | Best for multi-class | Expensive computation |

---

# ‚≠ê Why Activation Functions Matter for Backpropagation?

Because backpropagation uses **derivatives**.
Activation functions must be:

* Differentiable
* Smooth
* Non-linear

ReLU, Sigmoid, Tanh ‚Üí all have derivatives ‚Üí so backprop can update weights.

---