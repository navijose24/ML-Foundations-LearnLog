# â­ **ACTIVATION FUNCTIONS**


# ğŸ’¡ **What is an Activation Function?**

An activation function decides whether a neuron should â€œfireâ€ or not.

It introduces **non-linearity** â†’ this allows neural networks to learn **complex patterns**.

Without activation functions â†’ your network becomes a simple linear model â†’ totally useless for real-world problems.

---

# ğŸ§  **Why We Need Activation Functions?**

Because real-world data is **non-linear**:

* Images
* Speech
* Emotions
* Natural language
* Business patterns

Activation functions create **curves and bends** so the network can learn the real patterns.

---

# â­ TYPES OF ACTIVATION FUNCTIONS


# 1ï¸âƒ£ **Sigmoid Function (Logistic)**

**Formula:**

$f(x) = \frac{1}{1 + e^{-x}}$

**Range:** 0 to 1
**Shape:** S-shaped curve

![Sigmoid Function](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQovfM4UAyi_a07pWKNPQpX8_JFU5851Gk_VO8bMpWptb6GIh9YMlMhecNIFUNf3TbAVezNUuF_)

### **Where to use:**

âœ” Binary classification output layer

âœ” Probabilities (0â€“1)

### **Advantages:**

* Smooth curve
* Output is probability-like

### **Disadvantages:**

* Vanishing gradient problem
* Saturates at extreme ends
* Slow learning

---

# 2ï¸âƒ£ **Tanh (Hyperbolic Tangent)**

**Formula:**

$f(x) = \tanh(x)$

**Range:** â€“1 to +1


![Tanh (Hyperbolic Tangent)](https://miro.medium.com/v2/resize:fit:756/1*tOc--h-QU9_bHqWLPY9YLA.png)


### **Where to use:**

âœ” Hidden layers (better than sigmoid)

### **Advantages:**

* Zero-centered output
* Stronger gradient than sigmoid

### **Disadvantages:**

* Still has vanishing gradient

---

# 3ï¸âƒ£ **ReLU (Rectified Linear Unit)**

**Formula:**

$f(x) = \max(0, x)$

![ReLu](https://cdn.prod.website-files.com/614c82ed388d53640613982e/64a6c16cff99b6337d9dacf7_parametric%20relu%20and%20first%20derivative.webp)

### **Where to use:**

âœ” All modern deep neural networks

âœ” CNNs, RNNs, Transformers

### **Advantages:**

* No vanishing gradient for x > 0
* Simple and fast
* Best for hidden layers

### **Disadvantages:**

* Dead ReLU problem (neurons stop updating if x < 0 always)

---

# 4ï¸âƒ£ **Leaky ReLU**

Fixes ReLUâ€™s â€œdead neuronâ€ problem.

**Formula:**
$$
f(x) =
\begin{cases}
x & x > 0 \
0.01x & x \le 0
\end{cases}
$$


![leaky ReLu](https://cdn.analyticsvidhya.com/wp-content/uploads/2017/10/prelu-300x262.png)

### **Where to use:**

âœ” When ReLU neurons die

âœ” Deep networks

---

# 5ï¸âƒ£ **Softmax**

Used only in **multi-class classification (output layer)**.

**Formula:**

$\sigma(z_i)=\frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}$

![soft max](images/softmax.png)

### **Where to use:**

âœ” Multi-class output (cat, dog, horse)

âœ” Probability distribution (sums to 1)

---

# ğŸ“Œ In short:

| Activation     | Range   | Use Case           | Pros                 | Cons                  |
| -------------- | ------- | ------------------ | -------------------- | --------------------- |
| **Sigmoid**    | 0 to 1  | Binary output      | Probability          | Vanishing gradient    |
| **Tanh**       | â€“1 to 1 | Hidden layers      | Zero-centered        | Can still saturate    |
| **ReLU**       | 0 to âˆ  | Hidden layers      | Fast, no VG          | Dead neurons          |
| **Leaky ReLU** | â€“âˆ to âˆ | Deep hidden layers | Fixes dead ReLU      | Slight noise          |
| **Softmax**    | 0â€“1     | Multi-class output | Best for multi-class | Expensive computation |

---

# â­ Why Activation Functions Matter for Backpropagation?

Because backpropagation uses **derivatives**.
Activation functions must be:

* Differentiable
* Smooth
* Non-linear

ReLU, Sigmoid, Tanh â†’ all have derivatives â†’ so backprop can update weights.

---