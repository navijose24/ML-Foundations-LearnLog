# â­ **BACKPROPAGATION ALGORITHM**

(The heart of training neural networks)

ğŸ’¡ **What is Backpropagation?**

Backpropagation = **how a neural network learns** by correcting its mistakes.

It is a **learning algorithm** used to update weights using:

1. **Forward pass** â†’ make prediction
2. **Find error** (difference between predicted & actual)
3. **Backward pass** â†’ send error back through the network
4. **Update weights** using gradient descent

It is basically:
ğŸ‘‰ â€œI predicted wrong â†’ let me adjust my weights â†’ try againâ€

---


âš™ï¸ **WHY do we need backprop?**

Because in multilayer networks (many hidden layers), we canâ€™t manually find how much *each* weight contributed to the error.

Backprop tells us:
â€œHow responsible was each weight for the final error?â€

Then it updates that weight accordingly.

---


ğŸ§± **STEPS OF BACKPROPAGATION**

Full pipeline clearly.

---


âœ” **STEP 1: Forward Propagation**

Input â†’ Hidden layer â†’ Output
Compute:

$z = w_ix_i + b$

$a = f(z)$

You get a prediction Å·.

---


âœ” **STEP 2: Compute Error (Loss)**

Most common loss:


**Mean Squared Error (MSE)**

$E = \frac{1}{2}(t - y)^2$

Where
t = true output
y = predicted output

---


âœ” **STEP 3: Backward Propagation (Chain Rule)**

You compute how much **each weight** contributed to the error.

This is done using **partial derivatives**:

$\frac{\partial E}{\partial w}$

This derivative tells us:
â€œIf I change the weight w by a little, how much will the error change?â€

---

## âœ” **STEP 4: Weight Update Rule**

Using **Gradient Descent**:

$w_{new} = w_{old} - \eta \frac{\partial E}{\partial w}$

Where
$Î· (eta)$ = learning rate

This reduces the error.

---

# ğŸ§  **INTUITIVE**

Imagine you are throwing a dart and miss the target.

Forward pass â†’ You throw the dart
Loss â†’ The distance from the center
Backward pass â†’ Someone tells you â€œmove your hand slightly leftâ€
Weight update â†’ You adjust your position
Repeat â†’ Throw until you hit the center

Same idea in neural networks.

---

# â­ EXAMPLE 

Consider 1 neuron with sigmoid activation.

**Forward pass:**

1. Compute weighted sum z
2. Apply sigmoid
3. Get prediction y


**Backward pass:**

Compute:

1. Derivative of loss wrt output
2. Derivative wrt activation
3. Derivative wrt weight

Finally update weight.

---

ğŸ¯ **Why Chain Rule?**

Because in multi-layer networks:

Input â†’ Hidden â†’ Hidden â†’ Output

Each layer affects the next.
Chain rule tells how error flows **layer-by-layer backwards**.

---

# ğŸ“ In short:


â€œBackpropagation is a supervised learning algorithm used to train multilayer neural networks by propagating the error backward and updating the weights using gradient descent.â€

**Key Steps:**

1. **Forward pass** â€“ compute output
2. **Compute error**
3. **Backward pass** â€“ compute gradients using chain rule
4. **Update weights** using gradient descent

 **Advantages:**

âœ” Efficient for deep networks

âœ” Computes exact gradients

âœ” Allows training multi-layer networks

âœ” Works with any differentiable activation function

**Disadvantages:**

âœ˜ Requires differentiable functions

âœ˜ Suffers from vanishing gradients (sigmoid/tanh)

âœ˜ Slow if too many layers

âœ˜ Sensitive to learning rate

**Applications:**

* Deep learning
* Image classification
* Speech recognition
* Time-series forecasting
* Medical diagnosis
* NLP (transformers, RNNs, etc.)

---