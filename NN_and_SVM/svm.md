# â­ **SUPPORT VECTOR MACHINES (SVM)**


# ğŸ”¥ 1. What is SVM? 

SVM is a **supervised classification algorithm** that tries to:

ğŸ‘‰ **Find the best separating boundary (hyperplane)**

ğŸ‘‰ **That maximizes the distance (margin)**

ğŸ‘‰ **Between the classes**

It is basically trying to draw the **safest possible boundary** so that the classes are **far apart**.

---

# â­ 2. Why is SVM special?

Most algorithms just find *any* boundary.

But SVM finds the boundary that is:

âœ” Maximum Margin

âœ” Most robust

âœ” Least error-prone

Because a bigger margin = better generalization on unseen data.

---

# ğŸ” 3. Important Terms

## **Hyperplane**

The decision boundary that separates classes.

For 2D â†’ it's a line
For 3D â†’ it's a plane
For nD â†’ it's a hyperplane

---

## **Margin**

Distance between hyperplane and the closest data points.

---

## **Support Vectors**

The **data points that lie closest to the boundary**.
These are the â€œcriticalâ€ points that define the hyperplane.

If you remove them â†’ the boundary changes.

---

# â­ 4. Maximum Margin Classification (Math Intuition)

SVM wants to maximize:

$\text{Margin} = \frac{2}{||w||}$

So SVM tries to **minimize**:

$||w||^2$

Subject to each point being correctly classified.

---

# ğŸ”¥ 5. Types of SVM

## **1ï¸âƒ£ Hard Margin SVM**

Assumes:

* Data is perfectly linearly separable
* No misclassification allowed

Not practical for real-world data.

---

## **2ï¸âƒ£ Soft Margin SVM**

Allows **some misclassifications**
Uses a penalty parameter **C**

* Large C â†’ less misclassification, tighter margin
* Small C â†’ larger margin, more tolerance

Used in real life.

---

# â­ 6. Non-linear SVM

What if data is **not linearly separable**?

Example:

â­•â­•â­• (class 1 in circle)

ğŸŸ¦ğŸŸ¦ğŸŸ¦ (class 2 around it)

A straight line canâ€™t separate them.

### Solution?

ğŸ‘‰ Transform data into higher dimensions.

This is where **Kernels** come in.

---

# ğŸŒˆ 7. Kernel Trick

SVM uses a **kernel function** to project data into a higher dimension **without actually computing it manually**.

This makes SVM extremely powerful.

---

# â­ 8. Types of Kernels

### **1. Polynomial Kernel**

$K(x,y) = (x^Ty + 1)^d$

Makes boundaries curved.

Use when:
âœ” Data has curved decision boundaries

âœ” Degree d controls flexibility

---

### **2. RBF (Radial Basis Function) Kernel**

$K(x,y)= e^{-\gamma ||x-y||^2}$

Most popular kernel.

Use when:
âœ” Decision boundary is complex

âœ” Data is highly non-linear

âœ” You need flexibility

---

### **3. Linear Kernel**

[
K(x,y) = x^T y
]

Good for:
âœ” High-dimensional data

âœ” Text classification

âœ” Sparse data

---

# ğŸ¯ 9. Advantages of SVM

âœ” Works well on small and medium datasets

âœ” Great for high-dimensional data

âœ” Effective for non-linear data using kernels

âœ” Robust to overfitting

âœ” Convex optimization (unique global solution)

---

# âŒ 10. Disadvantages

âœ˜ Slow for very large datasets

âœ˜ Choosing kernel & parameters is tricky

âœ˜ Not suitable for noisy datasets

âœ˜ Hard to interpret

---

# ğŸ§  11. Applications

* Face detection
* Bioinformatics
* Handwriting recognition
* Text classification
* Image classification
* Fraud detection

---
