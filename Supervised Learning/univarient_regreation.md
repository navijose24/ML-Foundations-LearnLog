# ‚≠ê **1. LINEAR REGRESSION WITH ONE VARIABLE (UNIVARIATE REGRESSION)**

This is the **simplest** supervised learning algorithm.



# üí° **What is Linear Regression?**

It predicts a **continuous numerical value** using a straight line.

Examples:

* Predict house price based on size
* Predict marks based on study hours
* Predict weight based on height

You have **one input x** and **one output y**.

---

# ‚≠ê **Hypothesis Function**

Linear regression fits a line:

$h_\theta(x) = \theta_0 + \theta_1 x$

* $(\theta_0)$ = intercept
* $(\theta_1)$ = slope
* x = input
* output = prediction

This line should best fit the training data.

---

# ‚≠ê **Cost Function ‚Äî Mean Squared Error (MSE)**

This tells how ‚Äúbad‚Äù the line is.

$J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2$

Why squared error?

‚úî Punishes large errors more
‚úî Smooth curve ‚Üí easy to optimize
‚úî Mathematical convenience

Why divided by 2?
‚Üí Makes derivative simpler.

---

# ‚≠ê **Goal of Linear Regression**

Find parameters $(\theta_0, \theta_1)$ that **minimize** the cost function.

This is classic optimization.

---

# ‚≠ê **How to Minimize Cost?**

Two important methods:

‚úî 1. Gradient Descent (iterative)

 ‚úî 2. Normal Equation (direct matrix solution)


---

# ‚≠ê **Gradient Descent Intuition**

Imagine you are on a mountain in the dark.
You want to reach the valley (minimum cost).
You take small steps downhill.

The slope (derivative) tells you the direction.

---

# ‚≠ê **Derivative (Learning Rule)**

$\theta_j := \theta_j - \alpha \frac{1}{m}\sum (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}$

* Œ± = learning rate
* Repeat until convergence

Too large Œ± ‚Üí diverges
Too small Œ± ‚Üí very slow

---

# ‚≠ê **Geometric Interpretation**

Linear regression tries to find the **best straight line** that minimizes total error distance.

---

# ‚≠ê **Assumptions of Linear Regression**


1Ô∏è‚É£ Linearity: relationship between x and y is linear
2Ô∏è‚É£ Independence: observations independent
3Ô∏è‚É£ Homoscedasticity: equal variance
4Ô∏è‚É£ Normal distribution of errors
5Ô∏è‚É£ No multicollinearity (for multiple regression)

For **one variable**, only linearity and independence matter.

---

# ‚≠ê In short:

**Linear regression with one variable is the simplest supervised learning algorithm used to predict a continuous output. It assumes a linear relationship between the input variable x and output y. The hypothesis function is a straight line defined as (h_\theta(x)=\theta_0+\theta_1x). The parameters (\theta_0) and (\theta_1) are learned by minimizing the cost function, which is Mean Squared Error (MSE). The MSE is given by:**


$$J(\theta_0, \theta_1)=\frac{1}{2m}\sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2$$

**To find the parameter values that minimize the cost, gradient descent is used. Gradient descent iteratively updates the parameters using the formula:**


$$\theta_j := \theta_j - \alpha \frac{1}{m}\sum (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$$


**This process continues until convergence. Linear regression is widely used due to its simplicity, interpretability, and efficiency.**


