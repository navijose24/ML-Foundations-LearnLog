# â­ **K-Means Clustering **

K-Means is the **most popular unsupervised clustering algorithm** used to group data into **K clusters**.

---

# 1ï¸âƒ£ **What is the goal of K-Means?**

To divide data into **K groups** such that:

* Points **inside a cluster** are **similar**
* Points **across clusters** are **different**

It minimizes:

### **Within-Cluster Sum of Squared Error (WCSS)**

$\sum (x_i - \mu_k)^2$

---

# 2ï¸âƒ£ **How K-Means Works (Algorithm Steps)**

### **Step 1: Choose K**

Decide how many clusters you want.

### **Step 2: Initialize centroids**

Randomly pick K points as initial â€œcentersâ€.

### **Step 3: Assignment Step**

Assign each data point â†’ closest centroid using **Euclidean distance**.

$\text{cluster}(x) = \arg\min_k ; d(x, \mu_k)$

### **Step 4: Update Step**

Recompute each centroid by taking the **mean** of points in that cluster.

$\mu_k = \frac{1}{N_k}\sum_{\text{points in }k} x_i$

### **Step 5: Repeat until convergence**

Converges when:

* Centroids stop changing, or
* Assignments donâ€™t change

---

# 3ï¸âƒ£ **Intuition (Why it works)**

K-Means tries to put each point close to a â€œcenterâ€.
Centers keep moving until everything settles.

Think of it like:
**People grouping around different leaders, and the leaders shifting to the center of their group.**

---

# 4ï¸âƒ£ **Advantages**

âœ” Simple & fast

âœ” Works well on large datasets

âœ” Easy to interpret

âœ” Always converges

---

# 5ï¸âƒ£ **Disadvantages**

âŒ Needs K in advance

âŒ Sensitive to initial centroids

âŒ Sensitive to outliers

âŒ Only works for spherical / convex clusters

---

# 6ï¸âƒ£ **When to use K-Means?**

Use when:

* Data is **numeric & continuous**
* Clusters are round-shaped
* Dataset is large (1000+ points)
* You want fast unsupervised grouping

---

# 7ï¸âƒ£ **When NOT to use K-Means?**

Avoid when:

* Data contains outliers
* Clusters are not spherical
* Clusters have very different sizes

---

# 8ï¸âƒ£ **Practical**

ðŸ“Œ Always scale data (StandardScaler).
ðŸ“Œ Try multiple K using the **Elbow Method**.
ðŸ“Œ Use **k-means++** for better initialization.
ðŸ“Œ Run multiple times and pick best inertia.

---

### In short:

> K-Means is a partitional, centroid-based clustering algorithm that partitions data into K clusters. It minimizes the Within-Cluster Sum of Squared Error (WCSS). 
The algorithm repeats two steps: 
    (1) Assignment â€” assign each point to the nearest centroid using Euclidean distance, and 
    (2) Update â€” recompute centroids as the mean of all points in the cluster. The process continues until convergence.
>
> Advantages: simple, fast, works well on large datasets.
> Disadvantages: requires K, sensitive to initial values and outliers.
>
> Used for unsupervised grouping of continuous numeric data.

---
