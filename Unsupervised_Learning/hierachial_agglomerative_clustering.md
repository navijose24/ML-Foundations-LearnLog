# â­ **Hierarchical Agglomerative Clustering (HAC)**



# 1ï¸âƒ£ **What is Hierarchical Clustering?**

A clustering method that builds a **tree-like structure** (called a **dendrogram**) to represent how data points are grouped.

There are two types:

### **1. Agglomerative (bottom-up)** 

Start with each point as a cluster â†’ keep merging.

### **2. Divisive (top-down)**

Start with one cluster â†’ keep splitting.


---

# 2ï¸âƒ£ **Agglomerative Clustering: Step-by-Step**



Start with **N clusters** (each point is its own cluster).

### **Step 1:** Compute distance matrix

Find pairwise distances between all points.

### **Step 2:** Find the closest two clusters

Pick the pair with minimum distance.

### **Step 3:** Merge them

Combine the two clusters into one.

### **Step 4:** Update the distance matrix

Update distances between new cluster vs others using **linkage methods**.

### **Step 5:** Repeat until only 1 cluster remains

This forms a **hierarchy of clusters**, visualized using a **dendrogram**.

---

# 3ï¸âƒ£ **Linkage Methods **

These methods decide **how to measure distance between two clusters**.

### âœ” **1. Single Linkage**

Distance = minimum distance between any two points in the clusters.

$D(A,B) = \min_{i \in A, j \in B} d(i,j)$

**Effect:** Can form long, chain-like clusters (**chaining effect**).

---

### âœ” **2. Complete Linkage**

Distance = maximum distance between points.

$D(A,B) = \max_{i \in A, j \in B} d(i,j)$

**Effect:** Produces compact, tight clusters.

---

### âœ” **3. Average Linkage**

Distance = average distance between all pairs.

$D(A,B) = \text{avg}_{i \in A, j \in B} d(i,j)$

**Effect:** Balanced clusters (between single & complete).

---

### âœ” **4. Wardâ€™s Method**

Merges clusters that result in the **smallest increase in WCSS**.

**Effect:** Works well for numeric data (very popular).

---

# 4ï¸âƒ£ **Advantages**

âœ” No need to choose K in advance

âœ” Produces dendrogram (full hierarchy)

âœ” Works for any shape clusters

âœ” Good for small datasets

---

# 5ï¸âƒ£ **Disadvantages**

âŒ Computationally expensive (O(nÂ²))

âŒ Cannot undo merges (greedy algorithm)

âŒ Sensitive to noise & outliers

âŒ Hard for very large datasets

---

# 6ï¸âƒ£ **When to Use HAC?**

Use when:

* You want a **visual hierarchy** (dendrogram).
* Dataset is **small/medium**.
* You donâ€™t know how many clusters to choose.
* You need explainability.

---

# 7ï¸âƒ£ **Dendrogram**

A tree diagram showing how clusters are merged.

Use it to:

* Choose number of clusters
* Understand cluster relationships

You â€œcutâ€ the dendrogram horizontally to get K clusters.

---

# ðŸ“Œ In short:

> **Hierarchical agglomerative clustering is a bottom-up clustering technique where each data point starts as its own cluster, and the algorithm repeatedly merges the closest pair of clusters based on a similarity measure. The process is repeated until all points belong to a single cluster, forming a hierarchical structure called a dendrogram.
>
> The distance between clusters can be defined using linkage methods: Single linkage (minimum distance), Complete linkage (maximum distance), Average linkage (mean distance), and Wardâ€™s method (minimizes increase in within-cluster variance).
>
> Advantages: no need to choose K, produces dendrogram, works for different cluster shapes.
> Disadvantages: high computational cost, irreversible merges, sensitive to noise.**



---
