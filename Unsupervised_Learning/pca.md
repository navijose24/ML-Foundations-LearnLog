# â­ **Principal Component Analysis (PCA)**

Dimensionality Reduction â†’ Unsupervised Learning

---

# 1ï¸âƒ£ **What is PCA?**

PCA is a mathematical technique that:

ðŸ‘‰ **Reduces the number of features**
ðŸ‘‰ **Keeps maximum possible information**
ðŸ‘‰ **Transforms data into new axes called Principal Components**

These new axes capture the most variance in the data.

---

# 2ï¸âƒ£ **Why do we use PCA?**


âœ” Remove redundant or correlated features

âœ” Reduce noise

âœ” Visualize high-dimensional data

âœ” Speed up ML algorithms

âœ” Prevent overfitting

âœ” Makes clustering/ML more stable

---

# 3ï¸âƒ£ **Intuition**

Imagine a dataset that lies along a **diagonal line**.
But your axes (x, y) are horizontal and vertical â€” not aligned with the data.

PCA **rotates** the axes:

* PC1 â†’ direction of maximum spread
* PC2 â†’ direction of remaining spread

So PCA finds **best directions** to represent your data.

---

# 4ï¸âƒ£ **How PCA Works â€” Step By Step**

**Step 1: Standardize the data**

Mean = 0, variance = 1
Why?
Because PCA is affected by scale.

**Step 2: Compute the Covariance Matrix**
$C = \frac{1}{n-1}X^TX$

This matrix shows how features vary together.

**Step 3: Compute Eigenvalues & Eigenvectors of Covariance Matrix**

* **Eigenvalues** â†’ importance (variance explained)
* **Eigenvectors** â†’ directions (principal components)

**Step 4: Sort Eigenvalues in descending order**

PC1 = highest eigenvalue
PC2 = second highest
PCk = kth eigenvalue

Higher eigenvalue = more variance.

**Step 5: Select top K components**

Choose K such that the cumulative variance is enough (e.g., 95%).

**Step 6: Transform data onto the new subspace**

$Y = X \cdot W$

Where W = matrix of selected eigenvectors.

This gives us **reduced-dimensional data**.

---

# 5ï¸âƒ£ **Variance **

The proportion of variance captured by each component:

$\text{Explained variance} = \frac{\lambda_i}{\sum \lambda}$

Used to choose number of components.

---

# 6ï¸âƒ£ **Advantages**

âœ” Reduces dimensionality efficiently

âœ” Removes noise

âœ” Makes algorithms faster

âœ” Handles correlated features

âœ” Helps visualization (2D, 3D)

---

# 7ï¸âƒ£ **Disadvantages**

âŒ Components lose original meaning

âŒ Linear method â†’ canâ€™t catch non-linearity

âŒ Sensitive to scaling

âŒ Sensitive to outliers

---

# 8ï¸âƒ£ **Where PCA is used?**

âœ” Image compression

âœ” Face recognition

âœ” Data visualization

âœ” Noise reduction

âœ” Preprocessing before clustering/classification

---

# 9ï¸âƒ£ In short:

> **PCA is a dimensionality reduction technique that transforms data into a new coordinate system such that the greatest variance lies on the first principal component, the second greatest on the second component, and so on.
>
> Steps: (1) Standardize the data. (2) Compute the covariance matrix. (3) Calculate eigenvalues and eigenvectors. (4) Sort eigenvalues in descending order. (5) Select top K eigenvectors. (6) Transform the original data onto the new reduced space.
>
> PCA reduces redundancy, increases efficiency, and is widely used for visualization, noise reduction, and preprocessing before clustering.**



---